{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HumanPartSegNet.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0qzmBSIBxFr",
        "colab_type": "text"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOxI_hEDksDe",
        "colab_type": "code",
        "outputId": "4f1b26f2-773a-43e3-cd07-1b8e450abc27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "!pip install torch torchvision"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.3.1)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.4.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.17.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.12.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (4.3.0)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.1.1->torchvision) (0.46)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXqiuWGSB4s2",
        "colab_type": "code",
        "outputId": "ad0e19ba-865a-4c69-97ad-b42ecc73649c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oF42ee-VlA0S",
        "colab_type": "text"
      },
      "source": [
        "#The model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFXkWLYnlGqt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "import numpy as np\n",
        "\n",
        "class SegNet(nn.Module):\n",
        "\n",
        "  def conv(self, in_channels, out_channels, kernel_size=3, padding=1):\n",
        "    return nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size, padding=padding), nn.BatchNorm2d(out_channels), nn.ReLU())\n",
        "\n",
        "  def skip_conv(self, in_channels, out_channels):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(in_channels, out_channels, 1), nn.BatchNorm2d(out_channels))\n",
        "\n",
        "  def upconv(self, in_channels, out_channels):\n",
        "    return nn.Sequential(\n",
        "        nn.ConvTranspose2d(in_channels, out_channels, 4, stride=2),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "\n",
        "  # Assume for simplicity that images are square\n",
        "  def crop_to_size(self, input, out_size):\n",
        "    crop_start = (input.size()[2] - out_size[2]) // 2\n",
        "    crop_end = int(np.ceil((input.size()[2] - out_size[2]) / 2))\n",
        "    return input[:, :, crop_start:-crop_end, crop_start:-crop_end]\n",
        "\n",
        "  def __init__(self, out_channels, in_channels=3, feature_extract=True):\n",
        "    super(SegNet, self).__init__()\n",
        "\n",
        "    self.vgg = models.vgg16_bn(pretrained=True).features\n",
        "\n",
        "    # Make the first conv layer have padding of 100\n",
        "    new_conv = nn.Conv2d(in_channels, 64, 3, padding=100)\n",
        "    new_conv.weight = self.vgg[0].weight\n",
        "    new_conv.bias = self.vgg[0].bias\n",
        "    self.vgg[0] = new_conv\n",
        "\n",
        "    # Using VGG as a fixed feature extractor so we shouldn't optimize its weights\n",
        "    if feature_extract:\n",
        "      for param in self.vgg.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    self.fc7_conv = nn.Sequential(\n",
        "        self.conv(512, 4096, kernel_size=7, padding=0),\n",
        "        self.conv(4096, 4096, kernel_size=1, padding=0)\n",
        "    )\n",
        "\n",
        "    self.upconv1 = self.upconv(4096, out_channels)\n",
        "    self.upconv2 = self.upconv(out_channels, out_channels)\n",
        "    self.upconv3 = self.upconv(out_channels, out_channels)\n",
        "    self.upconv4 = self.upconv(out_channels, out_channels)\n",
        "    self.upconv5 = self.upconv(out_channels, out_channels)\n",
        "\n",
        "    self.skip_conv1 = self.skip_conv(512, out_channels)\n",
        "    self.dropout1 = nn.Dropout2d()\n",
        "    self.skip_conv2 = self.skip_conv(256, out_channels)\n",
        "    self.dropout2 = nn.Dropout2d()\n",
        "    self.skip_conv3 = self.skip_conv(128, out_channels)\n",
        "    self.dropout3 = nn.Dropout2d()\n",
        "    self.skip_conv4 = self.skip_conv(64, out_channels)\n",
        "    self.dropout4 = nn.Dropout2d()\n",
        "\n",
        "  def forward(self, x):\n",
        "    out1 = self.vgg[0:7](x)\n",
        "    out2 = self.vgg[7:14](out1)\n",
        "    out3 = self.vgg[14:24](out2)\n",
        "    out4 = self.vgg[24:34](out3)\n",
        "    out5 = self.fc7_conv(self.vgg[34:](out4))\n",
        "\n",
        "    upconv_out1 = self.upconv1(out5) \n",
        "    skip = self.crop_to_size(self.skip_conv1(out4), upconv_out1.size())\n",
        "    upconv_out1 = upconv_out1 + self.dropout1(skip)\n",
        "\n",
        "    upconv_out2 = self.upconv2(upconv_out1)\n",
        "    skip = self.crop_to_size(self.skip_conv2(out3), upconv_out2.size())\n",
        "    upconv_out2 = upconv_out2 + self.dropout2(skip)\n",
        "\n",
        "    upconv_out3 = self.upconv3(upconv_out2)\n",
        "    skip = self.crop_to_size(self.skip_conv3(out2), upconv_out3.size())\n",
        "    upconv_out3 = upconv_out3 + self.dropout3(skip)\n",
        "\n",
        "    upconv_out4 = self.upconv4(upconv_out3)\n",
        "    skip = self.crop_to_size(self.skip_conv4(out1), upconv_out4.size())\n",
        "    upconv_out4 = upconv_out4 + self.dropout4(skip)\n",
        "\n",
        "    out = self.upconv5(upconv_out4)\n",
        "    out = self.crop_to_size(out, x.size())\n",
        "\n",
        "    return out\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZK8m5ia256_X",
        "colab_type": "text"
      },
      "source": [
        "# Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bd2AkJwJ_PVw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#https://www.cs.stanford.edu/~roozbeh/pascal-parts/pascal-parts.html\n",
        "from torchvision import datasets, transforms\n",
        "import torchvision.transforms.functional as TF\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from PIL import Image\n",
        "from skimage import io\n",
        "\n",
        "from os.path import isfile, join\n",
        "from os import listdir as listdir\n",
        "\n",
        "import random\n",
        "\n",
        "class PascalPartsDataSet(Dataset):\n",
        "    \n",
        "    def __init__(self, img_dir, msk_dir, img_ext, msk_ext, augmentation=False):\n",
        "        self.img_dir = img_dir\n",
        "        self.msk_dir = msk_dir\n",
        "        self.img_ext = img_ext\n",
        "        self.msk_ext = msk_ext\n",
        "        self.fNames = [f.split('.')[0] for f in listdir(img_dir) if isfile(join(img_dir, f))]\n",
        "        self.augmentation = augmentation\n",
        "    \n",
        "    def data_augmentation(self, image, mask):\n",
        "      # Random crop\n",
        "      i, j, h, w = transforms.RandomResizedCrop.get_params(image, scale=(0.08, 1.0), ratio=(3. / 4., 4. / 3.))\n",
        "      image = TF.crop(image, i, j, h, w)\n",
        "      mask = TF.crop(mask, i, j, h, w)\n",
        "\n",
        "      # Random horizontal flip\n",
        "      if random.random() > 0.5:\n",
        "        image = TF.hflip(image)\n",
        "        mask = TF.hflip(mask)\n",
        "\n",
        "      # Random rotate and scale\n",
        "      scale = random.uniform(0.7, 1.4)\n",
        "      angle = random.randint(-30, 30)\n",
        "      image = TF.affine(image, angle, (0, 0), scale, 0)\n",
        "      mask = TF.affine(mask, angle, (0, 0), scale, 0)\n",
        "\n",
        "      # Random hue shift\n",
        "      hue = random.uniform(-0.1, 0.1)\n",
        "      image = TF.adjust_hue(image, hue)\n",
        "\n",
        "      return image, mask\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.fNames)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "    \n",
        "      img_name = self.fNames[idx] + self.img_ext\n",
        "      msk_name = self.fNames[idx] + self.msk_ext\n",
        "\n",
        "      img = Image.open(join(self.img_dir, img_name))\n",
        "      # We divide by 100 so that any class will be mapped to a number in the 0-1 range\n",
        "      mask = Image.fromarray(np.asarray(io.imread(join(self.msk_dir, msk_name))/100))\n",
        "\n",
        "      if self.augmentation:\n",
        "        img, mask = self.data_augmentation(img, mask)\n",
        "\n",
        "      img = TF.to_tensor(TF.resize(img, (224, 224)))\n",
        "      img = TF.normalize(img, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "\n",
        "      # Resize with nearest neighbour interpolation because we should not lerp between classes\n",
        "      mask = TF.to_tensor(TF.resize(mask, (224, 224), interpolation=Image.NEAREST))[0] * 100\n",
        "      mask = mask.long()\n",
        "\n",
        "      return (img, mask)\n",
        "\n",
        "\n",
        "def load_pascal_parts(parent_dir, batch_size):\n",
        "  transform = transforms.Compose([\n",
        "                transforms.Resize((224, 224), interpolation=0),\n",
        "                transforms.ToTensor()\n",
        "                ])\n",
        "  \n",
        "  train = PascalPartsDataSet(parent_dir + '/train/images', parent_dir + '/train/masks', '.jpg', '.png', augmentation=True)\n",
        "  test = PascalPartsDataSet(parent_dir + '/test/images', parent_dir + '/test/masks', '.jpg', '.png')\n",
        "\n",
        "  train_dloader = DataLoader(dataset=train, batch_size=batch_size, shuffle=True, num_workers=8)\n",
        "  test_dloader = DataLoader(dataset=test, batch_size=batch_size, shuffle=False, num_workers=8)\n",
        "\n",
        "  return train_dloader, test_dloader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVcxom1c_IDb",
        "colab_type": "text"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dL4oLafsgG5O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch import optim\n",
        "from tqdm import tqdm\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def train(lr, epochs, batch_size, parent_dir, check_point=None):\n",
        "\n",
        "  model = SegNet(7)\n",
        "  model.cuda()\n",
        "\n",
        "  model_params = [param for param in model.parameters() if param.requires_grad]\n",
        "  optimizer = optim.Adam(model_params, lr=lr)\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "  if check_point is not None:\n",
        "    model.load_state_dict(check_point['model_state_dict'])\n",
        "    optimizer.load_state_dict(check_point['optimizer_state_dict'])\n",
        "\n",
        "  train_dloader, test_dloader = load_pascal_parts(parent_dir, batch_size)\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "\n",
        "    running_loss = 0\n",
        "    for images, masks in tqdm(train_dloader):\n",
        "      images = images.cuda()\n",
        "      masks = masks.cuda()\n",
        "\n",
        "      output = model(images)\n",
        "\n",
        "      loss = criterion(output, masks)\n",
        "      \n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      \n",
        "      running_loss += loss.item()\n",
        "\n",
        "    print('Epoch: {} '.format(epoch), 'Training loss: ', running_loss / len(train_dloader))\n",
        "\n",
        "  checkpoint = {'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict()}\n",
        "\n",
        "  return model, checkpoint, train_dloader, test_dloader\n",
        "\n",
        "\n",
        "def load_model(cp_dir):\n",
        "  checkpoint = torch.load(cp_dir)\n",
        "\n",
        "  model = SegNet(7)\n",
        "  model.cuda()\n",
        "\n",
        "  model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uqtu3AWS8nnc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# parent_dir = 'drive/My Drive/Colab Notebooks/CSC420 Project/PASCAL_HumanParts/'\n",
        "parent_dir = 'drive/My Drive/CSC420 Project/PASCAL_HumanParts/'\n",
        "\n",
        "# cp_dir = 'drive/My Drive/Colab Notebooks/CSC420 Project/checkpoint'\n",
        "cp_dir = 'drive/My Drive/CSC420 Project/checkpoint'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amcvsfUw7TtK",
        "colab_type": "text"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4FSLymQ_MzU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cp = torch.load(cp_dir)\n",
        "\n",
        "!nvidia-smi -L # show gpu\n",
        "for i in range(10): # do 10 in a row and save after each 50 epochs just to be sure\n",
        "  model, cp, train_dloader, test_dloader = train(0.001, 50, 25, parent_dir, cp)\n",
        "  torch.save(cp, cp_dir)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnjyz70st0so",
        "colab_type": "text"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJDJfNZgN5oS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# Calculate IOU for each class and the average IOU over all classes\n",
        "# This computes and averages the IOUs over a batch\n",
        "def iou_metric(output, mask, num_classes, smooth=1e-6):\n",
        "\n",
        "  iou = np.array([])\n",
        "\n",
        "  for t in range(1, num_classes):\n",
        "    x = (output == t).int()\n",
        "    y = (mask == t).int()\n",
        "\n",
        "    intersection = torch.sum(x*y, (1, 2))\n",
        "    union = torch.sum(x + y, (1, 2)) - intersection\n",
        "    \n",
        "    iou = np.append(iou, torch.mean((intersection.float() + smooth) / (union.float() + smooth)).item())\n",
        "\n",
        "  iou = np.append(iou, np.mean(iou))\n",
        "\n",
        "  return iou*100\n",
        "\n",
        "\n",
        "# Evaluate the model on the test set using the IOU metric\n",
        "def evaluate(model, test_dloader, num_classes):\n",
        "  \n",
        "  model.eval()\n",
        "\n",
        "  iou = np.zeros(num_classes)\n",
        "\n",
        "  for images, masks in test_dloader:\n",
        "    images = images.cuda()\n",
        "    output = model(images)\n",
        "    _, predicted = torch.max(output.data, 1, keepdim=True)\n",
        "    predicted = predicted.cpu()\n",
        "\n",
        "    iou += iou_metric(predicted, masks, num_classes)\n",
        "\n",
        "  iou /= len(test_dloader)\n",
        "\n",
        "  return iou"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1H2yE4micmE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# show some resulting images\n",
        "def show_results(model, test_dloader, num_classes):\n",
        "  \n",
        "  model.eval()\n",
        "  mean = [0.485, 0.456, 0.406]\n",
        "  std = [0.229, 0.224, 0.225]\n",
        "  for images, masks in test_dloader:\n",
        "    images = images.cuda()\n",
        "    output = model(images)\n",
        "    _, predicted = torch.max(output.data, 1, keepdim=True)\n",
        "    predicted = predicted.cpu()\n",
        "    images = images.cpu()\n",
        "    images[:, 0, :, :] = images[:, 0, :, :] * std[0] + mean[0]\n",
        "    images[:, 1, :, :] = images[:, 1, :, :] * std[1] + mean[1]\n",
        "    images[:, 2, :, :] = images[:, 2, :, :] * std[2] + mean[2]\n",
        "    plt.imshow(TF.to_pil_image(images.cpu()[0]))\n",
        "    plt.show()  \n",
        "    plt.imshow(TF.to_pil_image(predicted[0] / 100.))\n",
        "    plt.show()\n",
        "    plt.imshow(TF.to_pil_image(masks[0] / 100.))\n",
        "    plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0lK8ugUPYFd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "segNet_model = load_model(cp_dir)\n",
        "\n",
        "train_dloader, test_dloader = load_pascal_parts(parent_dir, batch_size=25)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQJJ2kyXwofP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(evaluate(segNet_model, test_dloader, 7))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9OGadl6kEVr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "show_results(segNet_model, test_dloader, 7)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NO5_82KObJsL",
        "colab_type": "text"
      },
      "source": [
        "# Matching\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQVa3WQJgFdz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "segNet_model = load_model(cp_dir)\n",
        "vgg = models.vgg16_bn(pretrained=True).features\n",
        "vgg.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAtJw9KC-uJj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2 as cv\n",
        "from skimage.draw import line_aa, line\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def vgg_feature_match(image0, image1, segNet_model, vgg, patch_size=3 , keypoint_window=11, threshold = 100):\n",
        "  # makes sure image has 3 channels only and are 224 x 224\n",
        "  image0 = TF.to_tensor(TF.resize(image0, (224, 224)))[:3]\n",
        "  image1 = TF.to_tensor(TF.resize(image1, (224, 224)))[:3]\n",
        "  normalized_image0 = TF.normalize(image0, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]).unsqueeze(0).cuda()\n",
        "  normalized_image1 = TF.normalize(image1, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]).unsqueeze(0).cuda()\n",
        "\n",
        "  # Use VGG-16 up to the pooling layer\n",
        "  features_0 = vgg[:6](normalized_image0)[0].data.cpu()\n",
        "  features_1 = vgg[:6](normalized_image1)[0].data.cpu()\n",
        "\n",
        "  _, body_mask0 = torch.max(segNet_model(normalized_image0).data, 1, keepdim=True)\n",
        "  _, body_mask1 = torch.max(segNet_model(normalized_image1).data, 1, keepdim=True)\n",
        "\n",
        "  body_mask0 = body_mask0.cpu().squeeze()\n",
        "  body_mask1 = body_mask1.cpu().squeeze()\n",
        "\n",
        "  patch_offset = patch_size // 2\n",
        "\n",
        "  # A feature and match for each body part\n",
        "  feature_map_0 = [[] for i in range(6)]\n",
        "  feature_map_1 = [[] for i in range(6)]\n",
        "  all_matches = [[] for i in range(6)]\n",
        "\n",
        "  keypoint_window_offest = keypoint_window // 2\n",
        "  for i in range(keypoint_window_offest, 224 - keypoint_window_offest, keypoint_window):\n",
        "    for j in range(keypoint_window_offest, 224 - keypoint_window_offest, keypoint_window):\n",
        "      if (body_mask0[i,j] != 0):  # Ignore background pixels\n",
        "        # Require all pixels in a window to have the same class to consider the point\n",
        "        if(torch.sum(\n",
        "            body_mask0[i - keypoint_window_offest : i + keypoint_window_offest + 1, \n",
        "                       j - keypoint_window_offest : j + keypoint_window_offest + 1]) == (keypoint_window * keypoint_window * body_mask0[i,j])):\n",
        "          feature_map_0[body_mask1[i,j] - 1].append((i,j))\n",
        "\n",
        "        \n",
        "  for i in range(patch_offset, 224 - patch_offset - 1):\n",
        "    for j in range(patch_offset, 224 - patch_offset - 1):\n",
        "      if (body_mask1[i,j] != 0): feature_map_1[body_mask1[i,j] - 1].append((i,j))\n",
        "\n",
        "  for i in range(6):\n",
        "    feature_patch_matches = [features_1[:, x1 - patch_offset : x1 + patch_offset + 1, y1 - patch_offset : y1 + patch_offset + 1] for x1,y1 in feature_map_1[i]]\n",
        "    if (len(feature_patch_matches) > 0):          \n",
        "      for x0,y0 in feature_map_0[i]:\n",
        "        feature_patch0 = features_0[:, x0 - patch_offset : x0 + patch_offset + 1, y0 - patch_offset : y0 + patch_offset + 1]\n",
        "        responses = [torch.sum(np.abs(feature_patch_match - feature_patch0)) for feature_patch_match in feature_patch_matches]\n",
        "        match_idx = np.argmin(responses)\n",
        "        if (responses[match_idx] > threshold): continue #thresholding to prevent outliers\n",
        "        all_matches[i].append(((x0,y0),feature_map_1[i][match_idx]))\n",
        "\n",
        "  resultImage = torch.cat((image0.cpu(), image1.cpu()), axis=2)\n",
        "  result_mask = torch.cat((body_mask0.cpu(), body_mask1.cpu()),axis=1)\n",
        "  plt.imshow(TF.to_pil_image(resultImage))\n",
        "  plt.show()  \n",
        "  plt.imshow(result_mask)\n",
        "  plt.show()\n",
        "\n",
        "  resultImage = np.array(TF.to_pil_image(resultImage).convert('RGB'))\n",
        "  for i in range(len(all_matches)):\n",
        "    for match in all_matches[i]:\n",
        "      cv.line(resultImage, (match[0][1], match[0][0]), (match[1][1] + 224, match[1][0]), (255,0,0), 2)\n",
        "\n",
        "  plt.imshow(resultImage)\n",
        "  plt.show()\n",
        "\n",
        "  return resultImage\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oQHC12J5vSn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vidcap = cv.VideoCapture(join('drive/My Drive/CSC420 Project/', '170728_Berlin_D_023.mp4'))\n",
        "# vidcap = cv.VideoCapture(join('drive/My Drive/Colab Notebooks/CSC420 Project/', '170609_F_Varanasi_004.mp4'))\n",
        "\n",
        "success, image = vidcap.read()\n",
        "count = 0\n",
        "interval = 10\n",
        "skip = 50\n",
        "while success:\n",
        "  success, image0 = vidcap.read()\n",
        "  i = 0\n",
        "  while(success and i < interval):\n",
        "    success, image1 = vidcap.read()\n",
        "    i += 1\n",
        "\n",
        "  count += (interval + 1)\n",
        "\n",
        "\n",
        "  if count < skip : continue # skip first 300 frames\n",
        "  match = vgg_feature_match(Image.fromarray(cv.cvtColor(image0,cv.COLOR_BGR2RGB).astype('uint8'), 'RGB'), \n",
        "                            Image.fromarray(cv.cvtColor(image1,cv.COLOR_BGR2RGB).astype('uint8'), 'RGB'), segNet_model, vgg, patch_size=11, keypoint_window=11,threshold=500)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}